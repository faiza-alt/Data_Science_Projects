{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rusty Bargain used car sales service is developing an app to attract new customers. In that app, you can quickly find out the market value of your car. You have access to historical data: technical specifications, trim versions, and prices. You need to build the model to determine the value. \n",
    "\n",
    "Rusty Bargain is interested in:\n",
    "\n",
    "- the quality of the prediction;\n",
    "- the speed of the prediction;\n",
    "- the time required for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Vehicle Price Prediction Using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Necessary Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('/datasets/car_data.csv')\n",
    "\n",
    "# Overview of data\n",
    "print(data.info())\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Exploratory Data Analysis and Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and duplicates\n",
    "print(data.isnull().sum())\n",
    "print(f\"Number of duplicates: {data.duplicated().sum()}\")\n",
    "\n",
    "# Drop duplicates\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Inspect target variable\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(data['Price'], bins=50, kde=True)\n",
    "plt.title('Distribution of Price')\n",
    "plt.xlabel('Price')\n",
    "plt.show()\n",
    "\n",
    "# Remove extreme outliers in the Price column\n",
    "data = data[data['Price'].between(500, 100000)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "data = data.drop(['DateCrawled', 'DateCreated', 'NumberOfPictures', 'PostalCode', 'LastSeen'], axis=1)\n",
    "\n",
    "# Fill missing values\n",
    "data['VehicleType'].fillna('unknown', inplace=True)\n",
    "data['Gearbox'].fillna('unknown', inplace=True)\n",
    "data['FuelType'].fillna('unknown', inplace=True)\n",
    "data['NotRepaired'].fillna('unknown', inplace=True)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical_features = ['VehicleType', 'Gearbox', 'FuelType', 'Brand', 'NotRepaired', 'Model']\n",
    "numerical_features = ['RegistrationYear', 'Mileage', 'Power', 'RegistrationMonth']\n",
    "\n",
    "X = data.drop('Price', axis=1)\n",
    "y = data['Price']\n",
    "\n",
    "X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse_lr = mean_squared_error(y_test, y_pred_lr, squared=False)\n",
    "print(f\"Linear Regression RMSE: {rmse_lr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
    "print(f\"Random Forest RMSE: {rmse_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse_dt = mean_squared_error(y_test, y_pred_dt, squared=False)\n",
    "print(f\"Decision Tree RMSE: {rmse_dt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse_lgb = mean_squared_error(y_test, y_pred_lgb, squared=False)\n",
    "print(f\"LightGBM RMSE: {rmse_lgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse_xgb = mean_squared_error(y_test, y_pred_xgb, squared=False)\n",
    "print(f\"XGBoost RMSE: {rmse_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "cat_model = CatBoostRegressor(random_state=42, verbose=0)\n",
    "cat_model.fit(X_train, y_train)\n",
    "y_pred_cat = cat_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse_cat = mean_squared_error(y_test, y_pred_cat, squared=False)\n",
    "print(f\"CatBoost RMSE: {rmse_cat}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Regression\n",
    "\n",
    "\n",
    "Performance: RMSE = 3138.86\n",
    "Linear Regression provided the highest RMSE, indicating that it struggles to model the complex relationships in the data. This result is expected, as linear regression assumes linear relationships, which may not align with the nonlinear dependencies inherent in this dataset.\n",
    "\n",
    "Sanity Check: Despite its high error, Linear Regression is valuable as a baseline to compare the performance of more sophisticated models.\n",
    "\n",
    "\n",
    "2. Decision Tree\n",
    "\n",
    "\n",
    "Performance: RMSE = 2079.99\n",
    "Decision Tree showed improvement over Linear Regression, capturing some nonlinear relationships. However, it overfits easily due to its greedy nature, which limits its generalization on unseen data.\n",
    "\n",
    "Speed: Fast training time, but the simplicity of its splits leads to limited accuracy compared to ensemble methods.\n",
    "\n",
    "\n",
    "3. Random Forest\n",
    "\n",
    "\n",
    "Performance: RMSE = 1630.14\n",
    "Random Forest achieved a significant reduction in RMSE compared to Decision Tree. As an ensemble method, it combines multiple trees to improve accuracy and reduce overfitting.\n",
    "\n",
    "\n",
    "Strengths:\n",
    "Handles missing data and categorical features (via encoding) efficiently.\n",
    "Shows robust performance without much fine-tuning.\n",
    "\n",
    "\n",
    "Weaknesses: Longer training time compared to a single Decision Tree.\n",
    "\n",
    "\n",
    "\n",
    "4. LightGBM\n",
    "\n",
    "\n",
    "Performance: RMSE = 1750.12\n",
    "LightGBM is a gradient boosting model known for its efficiency. While its RMSE is slightly higher than Random Forest, it compensates with a much faster training speed and lower memory usage.\n",
    "\n",
    "\n",
    "Strengths:\n",
    "Built-in handling of categorical features, reducing preprocessing effort.\n",
    "Efficient for large datasets due to its leaf-wise tree growth.\n",
    "\n",
    "\n",
    "Weaknesses: Sensitive to hyperparameters; slightly underperformed compared to Random Forest.\n",
    "\n",
    "\n",
    "5. XGBoost\n",
    "\n",
    "\n",
    "\n",
    "Performance: RMSE = 1696.93\n",
    "XGBoost delivered better accuracy than LightGBM and marginally underperformed compared to CatBoost. It is highly customizable and supports regularization, which helps in handling overfitting.\n",
    "\n",
    "\n",
    "Strengths:\n",
    "Versatile and reliable with strong support for missing data and hyperparameter tuning.\n",
    "Often a go-to model for tabular data.\n",
    "\n",
    "\n",
    "Weaknesses:\n",
    "Requires One-Hot Encoding for categorical features, which increases data size and computational cost.\n",
    "Training time is higher compared to LightGBM.\n",
    "\n",
    "\n",
    "6. CatBoost\n",
    "\n",
    "\n",
    "Performance: RMSE = 1650.31\n",
    "CatBoost achieved the best performance in terms of RMSE. Its categorical feature encoding and robustness to overfitting make it a strong choice for structured data like this.\n",
    "\n",
    "\n",
    "Strengths:\n",
    "Handles categorical data natively without preprocessing, which simplifies pipeline development.\n",
    "Outperformed other models with minimal hyperparameter tuning.\n",
    "Fast training compared to XGBoost.\n",
    "\n",
    "\n",
    "Weaknesses:\n",
    "Slightly more resource-intensive than LightGBM.\n",
    "\n",
    "\n",
    "\n",
    "Conclusion\n",
    "\n",
    "Best Model: CatBoost was the top performer, achieving the lowest RMSE of 1650.31. Its ability to handle categorical features and generalize well makes it the best choice for predicting vehicle prices.\n",
    "\n",
    "\n",
    "Trade-offs:\n",
    "\n",
    "For a balance of speed and accuracy, Random Forest or LightGBM can be good alternatives.\n",
    "XGBoost offers strong performance but requires more preprocessing and computational cost.\n",
    "\n",
    "\n",
    "Insights:\n",
    "\n",
    "Ensemble methods (Random Forest, Gradient Boosting models) significantly outperform simpler models (Linear Regression, Decision Tree) by capturing complex patterns.\n",
    "\n",
    "Hyperparameter tuning, especially for gradient boosting models, can further improve results.\n",
    "Categorical feature encoding is critical; models like CatBoost and LightGBM simplify this process compared to XGBoost."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 3544,
    "start_time": "2024-11-21T12:30:11.866Z"
   },
   {
    "duration": 1928,
    "start_time": "2024-11-21T12:31:34.123Z"
   },
   {
    "duration": 344,
    "start_time": "2024-11-21T12:32:17.906Z"
   },
   {
    "duration": 518,
    "start_time": "2024-11-21T12:32:57.208Z"
   },
   {
    "duration": 2,
    "start_time": "2024-11-21T12:33:40.408Z"
   },
   {
    "duration": 10082,
    "start_time": "2024-11-21T12:34:04.469Z"
   },
   {
    "duration": 1690,
    "start_time": "2024-11-21T12:37:59.397Z"
   },
   {
    "duration": 1919,
    "start_time": "2024-11-21T12:38:01.089Z"
   },
   {
    "duration": 377,
    "start_time": "2024-11-21T12:38:03.011Z"
   },
   {
    "duration": 526,
    "start_time": "2024-11-21T12:38:03.391Z"
   },
   {
    "duration": 10361,
    "start_time": "2024-11-21T12:38:03.919Z"
   },
   {
    "duration": 352798,
    "start_time": "2024-11-21T12:38:14.282Z"
   },
   {
    "duration": 5686,
    "start_time": "2024-11-21T12:44:07.082Z"
   },
   {
    "duration": 4984,
    "start_time": "2024-11-21T12:44:12.771Z"
   },
   {
    "duration": 304694,
    "start_time": "2024-11-21T12:44:17.757Z"
   },
   {
    "duration": 27789,
    "start_time": "2024-11-21T12:49:22.453Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
